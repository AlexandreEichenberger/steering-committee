## Steering Committee Meeting 10/01/2021

### Attendance:

| Name (Affiliation)              | Present  |
| ------------------------------- | -------- |
| Prasanth Pulavarthi (Microsoft) | No      |
| Alexandre Eichenberger (IBM)    | Yes      |
| Mayank Kaushik (Nvidia)         | No       |
| Rajeev Nalawadi (Intel)         | Yes      |
| Wenming Ye (AWS)                | Yes      |

Others: Rajeev Rao (Nvidia), Ashwini Khade (Microsoft), Brad Messer, Jim Spohrer, Charles Vozkha, David Edelsohn (IBM)

### Agenda:
  
  #### Roadmap discussions
  - Fourth & final session today. 3 topics being covered
  - Will be recorded and posted on YouTube afterwards
  - Previous slides & recordings already available on ONNX Roadmap slack channel
  - Next week in SC meeting, there should be summary & decision/assignment to SIG/WG regarding next steps for each topic

  #### ONNX Model Zoo (Alex)
  - Organizing multiple models in the zoo with mix of newer & older opsets, models are still using ops that have been deprecated
  - Can we make it easy for users of the model zoo, should we organize in terms of legacy models vs newer ?
  - Ashwini suggested starting discussion with Wenbing Li who chairs the ONNX model zoo SIG, over time some converters have optimized the models and later versions are performant
  - In some cases ONNX runtime implementation can optimize as well
  - opening github issue would be good start
  - Matteo (ONNX contributor) has done changes to model version converters, need more help from community
  - There is a need to add adapters when new ops are added, CI tests available to verify the changes.
  - Correctness of ONNX format is being verified by CI tests

  #### ONNX Community workshop on Oct 21 (8 to 11am)
  - Presenters will be asked to provide the presentation & recording by Oct 15th
  - ONNX community presentations for workshop (10 presentations with 10 minute slots)
  
  - #1 ONNX Runtime Web: running your machine learning model in browser
  Speaker: Emma Ning (Microsoft)
  Abstract: ONNX Runtime Web (ORT Web), a new feature in ONNX Runtime to enable JavaScript developers to run and deploy machine learning models in browsers. It accelerates model inference in the browser on both CPUs and GPUs, through WebAssembly (WASM) and WebGL backends separately.

  - #2 ONNX as standard format for institution with legacy
  Speaker: Haixuan Xavier Tao ( Banque France)
  Abstract: ONNX format deeply impact the way Banque de France operate Artificial Intelligence. ONNX format enables great interoperability between ML/DL Framework which is great when you need to meet very specific legacy Institutional operations. ONNX format enables versatility in Infrastructure, whether it is internal, external cloud, on premise or at the edge.

  - #3 ONNX and the AI on IBM Z client journey
  Speaker(s): Elpida Tzortzatos & Andrew M. Sica (IBM)
  Abstract: IBM Z platform is home to some of the world's most critical workloads across a number of industries such as finance and retail. This session will describe our involvement with ONNX and how it facilitates the deployment of customer models. Our early experiences with the standard will be discussed, as well as how we see it helping to enable client scenarios.

  - #4 Intel® Neural Compressor: A Scalable Quantization Tool for ONNX Models
  Speaker: Mengni Wang (Intel)
  Abstract: Intel® Neural Compressor provides unified interfaces cross deep learning frameworks for popular network compression technologies, such as quantization, pruning, knowledge distillation. On quantization side, it provides an auto-tuning mechanism to help customers rapidly figure out the best low-precision solution on Intel hardware.We have used Intel® Neural Compressor to generate quantized Resnet50 and VGG16 models and upstream to ONNX Model Zoo (https://github.com/onnx/models). All enabled FP32 models in ONNX model zoo would have corresponding quantized modes through Intel® Neural Compressor, which will enrich the diversity and scale of INT8 models in ONNX Model Zoo.

  - #5 Ascend CANN and ONNX : inference interoperability for better performance
  Speaker: Zhipeng Huang (Mindspore)
  Abstract: Ascend is an ASIC AI accelerator developed by Huawei with CANN as its "CUDA" layer, and the hardware has achieved amazing performance in many AI benchmark. In this talk we will introduce how we utilize ONNX with CANN tooling for better inference interoperability and performance.

  - #6 Intel® OneAPI software stack: ONNX Support for xPU hardware
  Speaker: Kiefer Kuah (Intel)
  Abstract: Intel is developing a software stack for deploying ONNX models to utilize cutting edge features of latest xPU hardware thus delivering optimized performance across highly scaleable accelerator architectures spanning cloud/edge/client deployments

  - #7 ONNX: Boosting PaddlePaddle Deployment in Industry
  Speaker: Ti Zhou (Baidu)
  Abstract: PaddlePaddle has been widely used in the industry to improve the efficiency and quality of industrial production. In this talk, we will introduce how PaddlePaddle partners use PaddlePaddle models to achieve more flexible and efficient deployment in industrial production environments by leveraging ONNX.

  - #8 Auditing considerations for ONNX models and benchmarking with QuSandbox
  Speaker: Sri Krishnamurthy (QuantUniversity)
  Abstract: When it comes to auditing ONNX models, benchmarking and comparing against the gold-source models is important. For example, a model may have been built with MATLAB but will be deployed in production in ONNX. Through the QuSandbox, a rapid-prototyping and algorithmic assessment platform, we will discuss how considerations for testing, benchmarking and replication can be enabled to ensure robust assessments prior to deployment of models in production

  - #9 TVM: Dynamic shapes, control flow, and quantization with a compiler
  Speaker: Jason Knight (OctoML)
  Abstract: We have seen a rise in the use of dynamic shapes, control flow, and quantization in customer models at OctoML, and we'd like to share where ONNX has helped us and where we still perceive gaps. We'll also share an update as to what capabilities TVM has in these areas and how that drives further needs from ONNX moving forward.

  - #10: Place of ONNX in OpenVINO ecosystem
  Speaker: Sergey Lyalin (Intel)
  Abstract: Overview of capabilities that OpenVINO ecosystem provides for ONNX models inference. Role of ONNX as a key component between Pytorch and OpenVINO. Recent progress in ONNX operators coverage which makes OpenVINO a reliable solution for inference.

   
