# Roadmap for year 2020

Below are the summary of our roadmap discussions during year 2020.

## Data & ML pipeline

### Takuya Nakaike @IBM

- Dataframe as an input/output data type
- Dataframe such as pandas.DataFrame is very popular in machine learning (there are many use cases in Kaggle). There are three choices to enable dataframe in ONNX.
  - Extend sequence type. A sequence can be a collection of tensors. This looks like a dataframe assuming a tensor as a column. However, the types of elements in a sequence must be homogeneous. Since columns in a dataframe can have different types such as string and int, we need to extend sequence to have different types. Further, adding a label string to each tensor in a sequence is ideal to make an ONNX graph easy to understand.
  - Extend tensor list. Tensor list is proposed as a new input/output data type in the discussion about ONNX pipelines (https://gitter.im/onnx/pipelines, https://files.gitter.im/onnx/pipelines/yjFA/Onnx_Tensor_List_Discussion.pdf ). It is a list of tensors that can have different types. Same as extending sequence type, we would like to add a label string to each tensor in the list.
Introduce dataframe type.
  - New operators for ◊dataframe.
Since map/reduce/groupby operations to one or more columns are frequently used for feature engineering and preprocessing, we need to introduce new operators to support those operations. For example, mapping strings that match a regular expression to a common string (e.g, device A146, device B278 ⇒ device). Another mapping example is to concatenate strings in two columns into a string as a new feature.

- Preprocessing seconded by @snnn

### Sheng @ AWS
- Frameworks interoperability with numpy operator definitions
- Consider common abstraction for scheduling for more interoperability

### Yuankai Chen @ Alibaba
- Support various tensor layout formats. Today tensors are assumed to be in NCHW format. NCHW format is wided used with float32 data type, while other formats may be more performance efficient with int8 or float16 data types.

### Chin Huang @IBM
Define and develop an end-to-end ML pipeline scenario with Kubeflow to showcase complete AI application development lifecycle with ONNX


## Shape Inference

### @snnn (Changming)
- Re-organize the shape inference code a little bit, to make it easier to test and debug. They're too complex to be a lambda.
- Make the model-checker be able to detect shape inference errors. 

### @bhushan23 (Bhushan)
- Improving shape inference: More passes with known information and ability to re-run after optimizations
(E.g. After upsample, it’s hard time to optimize if scale is known and encoded outside ONNX)
- Ability to provide hints for shape inference if shape is unknown for certain op? (Not sure the generalize use case though)

## Operator Definitions

### @wschin (Wei-Sheng)
- Reduce the number of primitive operators. Some ONNX operators are composable using smaller primitive operators. If we can convert those composable operators to functions, runtime implementers’ life will be easier. This might be best to have inputs from Operator SIG.
- We currently don’t have a formal format for operator’s reference implementation, so different authors use different styles. It’d be good to have an universal format to increase maintainability and ambiguity.

### @gramalingam (Rama)
- Add reference implementation to ONNX. We have parts of this around, but currently this (partial) is scattered around in the test-case generator files. So, for example, it cannot be used to evaluate a sequence of ops (even if the reference implementation for each op is separately available).
- I second the suggestions from Wei-Sheng and Ke on reducing the number of primitive ops via layering. This is related and tied to the ongoing ONNX-MLIR work as well, so there may be some overlap here.
- Simplify the infrastructure for defining (the body of) functions. It is a bit cumbersome right now. Improve testing infrastructure for testing function definitions.

Reference implementations are required as part of adding new ops. However not structured in an usable fashion for running subgraphs. Also some older ops may be missing reference implementations. 
Can ONNX Runtime be the reference implementation? Won't have latest ops that are defined but not yet implemented.
Chin: should use an actual runtime rather than standalone python reference implementation
Ke, Joohoon: ONNX as a spec should not require reference implementation. just need rigorous compliance tests
Joohoon: make sure spec captures corner cases, rather than rely on reference implementation
Ke: root cause is spec is not clear enough
Ashwini: we've seen cases where english description is not enough
Ke: reference implementation is for generating compliance test data
Chin: Resize is an example of op with lots of corner cases

## IR

### @linkerzhang (Ke)
- Move ONNX from one “IR” to multiple “IRs”.
- That said, ONNX should not be taken as either an interoperable IR for hardware vendors or a DNN/ML frontend used to construct a computational graph. Especially, MLIR has provided a very good infrastructure to build IRs as needed in all areas. ONNX should target to build itself as a group of IRs (layered on each other) for ML/DNN (this vertical area)
- With the 1st above, ONNX community may provide centralized IR optimization libs (The current ONNX optimizer has not been maintained well :)). ONNX runtime has good ones, which may be contributed to the community, together with existing ones.

## Error handling

### @snnn
  - Fuzz the code.  Don't crash in any case. 
  - Consider making the code exception free. Not everyone wants exceptions. 

### @bhushan23
  - Better crash handling in ONNX runtime (Observed bunch of crashes for unsupported op configuration)
  
### @Adam Pocock 
  - Improve the robustness of the model checker and onnx protobuf loading code so it doesn’t suddenly terminate.
  Can ORT use the same checker as ONNX?
 
## Large model support  

### @snnn
  - Finish the support for large models(model size>=2GB). The current one has a few flaws that blocked the adaption. 

  
##  Model Zoo 

### @snnn
  - Currently the model tests only covered a few models from the ONNX model zoo, it should cover all. 
  - Model zoo should have code examples, and should be high quality. 
  make sure to show data loading code


### @Adam Pocock
  - Expand the model metadata schema generated from the onnx converters (or native saving). Saved ML models lack much of the necessary metadata needed to track things in production, like timestamps, model training algorithm, some notion of data source. Some of these will be hard to get from the information stored in a non-onnx model, but without a schema then the custom metadata field becomes converter dependent and tricky to parse if the information is available.
 
 Add more defined keys for use in the metadata field https://github.com/onnx/onnx/blob/master/docs/MetadataProps.md
  
## Quantization

### @snnn
  - Quantization specific optimizations e.g. https://github.com/pytorch/glow/blob/master/docs/Optimizations.md#quantization-specific-optimizations
may not be suitable for ONNX - better left for the runtimes

### @linkerzhang
  - Add representative quantized ONNX models into the ONNX model zoo. Of course, adding more ONNX quantization OPs if needed.
  Can we add HuggingFace quantized model?

## Language bindings
### @Adam Pocock, @Nick Pentreath
  - support other languages besides Python and C. Java in particular
  ONNX Runtime support Java, but ONNX tools (converters, model loaders, etc) do not
  Need Java8+ for use in Spark

