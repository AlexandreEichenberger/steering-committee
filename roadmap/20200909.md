# Op definition, shape inference, and IRs

## Shape Inference
### @snnn (Changming)
- Re-organize the shape inference code a little bit, to make it easier to test and debug. They're too complex to be a lambda.
- Make the model-checker be able to detect shape inference errors. 

### @bhushan23 (Bhushan)
- Improving shape inference: More passes with known information and ability to re-run after optimizations
(E.g. After upsample, it’s hard time to optimize if scale is known and encoded outside ONNX)
- Ability to provide hints for shape inference if shape is unknown for certain op? [Not sure the generalize use case though]

## Operator Definitions
### @wschin (Wei-Sheng)
- Reduce the number of primitive operators. Some ONNX operators are composable using smaller primitive operators. If we can convert those composable operators to functions, runtime implementers’ life will be easier. This might be best to have inputs from Operator SIG.
- We currently don’t have a formal format for operator’s reference implementation, so different authors use different styles. It’d be good to have an universal format to increase maintainability and ambiguity.

### @gramalingam (Rama)
- Add reference implementation to ONNX. We have parts of this around, but currently this (partial) is scattered around in the test-case generator files. So, for example, it cannot be used to evaluate a sequence of ops (even if the reference implementation for each op is separately available).
- I second the suggestions from Wei-Sheng and Ke on reducing the number of primitive ops via layering. This is related and tied to the ongoing ONNX-MLIR work as well, so there may be some overlap here.
- Simplify the infrastructure for defining (the body of) functions. It is a bit cumbersome right now. Improve testing infrastructure for testing function definitions.

## IR
### @linkerzhang (Ke)
- Move ONNX from one “IR” to multiple “IRs”.
- That said, ONNX should not be taken as either an interoperable IR for hardware vendors or a DNN/ML frontend used to construct a computational graph. Especially, MLIR has provided a very good infrastructure to build IRs as needed in all areas. ONNX should target to build itself as a group of IRs (layered on each other) for ML/DNN (this vertical area)
- With the 1st above, ONNX community may provide centralized IR optimization libs (The current ONNX optimizer has not been maintained well :)). ONNX runtime has good ones, which may be contributed to the community, together with existing ones.
